# Common environment variables for Spark services
x-spark-common-env:
  &spark-common-env
  SPARK_MASTER_HOST: spark-master
  SPARK_MASTER_PORT: 7077
  SPARK_MASTER_WEBUI_PORT: 8080
  SPARK_CONF_DIR: /opt/spark/conf

x-spark-image:
  &spark-image
  build:
    context: .
    dockerfile: Dockerfile
  image: spark-minio:latest

services:

  spark-master:
    <<: *spark-image
    container_name: spark-master

    volumes:
      - ${SPARK_PROJ_DIR:-.}/config:/opt/spark/conf
      - ${SPARK_PROJ_DIR:-.}/apps:/opt/spark-apps
      - spark_logs:/opt/spark/logs

    ports:
      - "8081:8080"
      - "7077:7077"
      - "6066:6066"

    environment:
      <<: *spark-common-env
      SPARK_DRIVER_MEMORY: 4g    # menos que 1g
      SPARK_EXECUTOR_MEMORY: 4g  # reducir tambiÃ©n el executor
      SPARK_JARS_PACKAGES: "org.postgresql:postgresql:42.7.1"

    command: >
      bash -c "mkdir -v -p /opt/spark/logs && \
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master \
      --host $$SPARK_MASTER_HOST \
      --port $$SPARK_MASTER_PORT \
      --webui-port $$SPARK_MASTER_WEBUI_PORT"

    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

    restart: always

  spark-worker:
    <<: *spark-image
    container_name: spark-worker

    volumes:
      - ${SPARK_PROJ_DIR:-.}/config:/opt/spark/conf
      - ${SPARK_PROJ_DIR:-.}/apps:/opt/spark-apps
      - spark_logs:/opt/spark/logs

    environment:
      <<: *spark-common-env
      SPARK_WORKER_CORES: "2"
      SPARK_WORKER_MEMORY: "4g"
      SPARK_JARS_PACKAGES: "org.postgresql:postgresql:42.7.1"

    command: >
      bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker \
      spark://$$SPARK_MASTER_HOST:$$SPARK_MASTER_PORT \
      --cores $$SPARK_WORKER_CORES \
      --memory $$SPARK_WORKER_MEMORY"

    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://spark-master:8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

    restart: always

    depends_on:
      spark-master:
        condition: service_healthy

  spark-history:
    image: apache/spark:4.0.1-python3
    container_name: spark-history

    environment:
      SPARK_HISTORY_OPTS: "-Dspark.history.fs.logDirectory=file:/opt/spark/logs"

    volumes:
      - ${SPARK_PROJ_DIR:-.}/config:/opt/spark/conf
      - spark_logs:/opt/spark/logs

    ports:
      - "18080:18080"

    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:18080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

    command: >
      bash -c "/opt/spark/sbin/start-history-server.sh && tail -f /dev/null"

    restart: always

    depends_on:
      spark-master:
        condition: service_healthy

volumes:
  spark_logs:
    external: true

networks:
  default:
    name: airflow_network
    external: true
